{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "2ayYjU5Kw1p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for the dataset file\n",
        "path_to_file = \"/content/ara_.txt\""
      ],
      "metadata": {
        "id": "5fX7UlcJw1mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Data Preprocessing:**\n",
        "The code begins with importing necessary libraries and defining functions for data preprocessing, including unicode_to_ascii and preprocess_sentence.\n",
        "The create_dataset function reads the dataset file, preprocesses the sentences, and returns word pairs for the specified number of examples.\n",
        "\n",
        "**2- Language Indexing and Tensor Generation:**\n",
        "The LanguageIndex class is defined to create an index for each language, including word-to-index and index-to-word mappings.\n",
        "The load_dataset function processes the word pairs, generates input and target tensors, and pads the sequences to a maximum length.\n",
        "\n",
        "**3- Training and Validation Split:**\n",
        "The code then splits the input and target tensors into training and validation sets using an 80-20 split.\n",
        "\n",
        "**4- Machine Translation using MBart Model:**\n",
        "It then imports the MBart model and tokenizer from the Hugging Face transformers library.\n",
        "The translate_with_mbart function takes a sentence in the source language, encodes it using the tokenizer, and generates the translated text using the MBart model.\n",
        "\n",
        "**5- Example Translation:**\n",
        "An example translation is performed using the MBart model for a given input sentence in Arabic, and the translated output is printed.\n"
      ],
      "metadata": {
        "id": "jyCE1ZxMzrLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Example 1:\n",
        "Input: \"Café\"\n",
        "Output: \"Cafe\"'''\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "b_F-3cUqw1im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Example :\n",
        "Input: \"Hello! How are you?\"\n",
        "Output: \"<start> hello ! how are you ? <end>\"'''\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z؀-ۿ?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    w = '<start> %s <end>' % w\n",
        "    return w"
      ],
      "metadata": {
        "id": "kCv3feXcw1gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='utf-8-sig').read().strip().split('\\n')\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
        "    print(len(lines))\n",
        "    print(len(lines[:num_examples]))\n",
        "    return word_pairs"
      ],
      "metadata": {
        "id": "Ny6oOyoMw1ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Example :\n",
        "# Create a LanguageIndex for English\n",
        "english_phrases = [\"hello world\", \"how are you\", \"world\"]\n",
        "english_index = LanguageIndex(english_phrases)\n",
        "print(english_index.word2idx)  # Output: {'<pad>': 0, 'are': 1, 'hello': 2, 'how': 3, 'world': 4, 'you': 5}\n",
        "print(english_index.idx2word)  # Output: {0: '<pad>', 1: 'are', 2: 'hello', 3: 'how', 4: 'world', 5: 'you'}'''\n",
        "\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word"
      ],
      "metadata": {
        "id": "yFVs_KHOw1ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''tensor_list = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
        "print(max_len)  # Output: 4'''\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "metadata": {
        "id": "9I1mJVFww1VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples):\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
        "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp, padding='post')\n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_tar, padding='post')\n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "metadata": {
        "id": "ff4pcthgw1St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "# Load the dataset and return Tensor of the input, Tensor for the target, Indexed input, Indexed target, Max length input, Max length target\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9blHaNOKw1QN",
        "outputId": "a4cd1255-f9d2-472a-fe78-12c8fcdd9ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10742\n",
            "10742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8fmZE5YxNx-",
        "outputId": "bc7bb7db-9bd5-4c1f-9a6a-172eb3f78d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8593, 8593, 2149, 2149)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer"
      ],
      "metadata": {
        "id": "ATCePjDzxNp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MBart model and tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYvze7ATxNnP",
        "outputId": "a3ed0ca0-3c36-4ac8-b504-faa752e7840e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
            "The class this function is called from is 'MBartTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Input Formatting:\n",
        "It formats the input sentence by prefixing it with the source language tag, creating the input text for translation.\n",
        "Encoding and Generation:\n",
        "It encodes the input text using the tokenizer to obtain input IDs.\n",
        "It generates the translated output using the MBart model, specifying the decoder start token ID for the target language.\n",
        "Decoding and Output:\n",
        "It decodes the generated tokens into the translated text using the tokenizer.'''\n",
        "\n",
        "# Define a function for translation using the MBart model\n",
        "def translate_with_mbart(sentence, source_lang, target_lang):\n",
        "    input_text = f\"{source_lang}: {sentence}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    translated = model.generate(input_ids, decoder_start_token_id=tokenizer.lang_code_to_id[target_lang])\n",
        "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "nMqyDl1JxNiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example translation using the MBart model\n",
        "source_lang = \"ar_AR\"  # Source language code (Arabic)\n",
        "target_lang = \"en_XX\"  # Target language code (English)\n",
        "example_sentence = \"مرحبا بالعالم\"  # Input sentence in Arabic\n",
        "translated_sentence = translate_with_mbart(example_sentence, source_lang, target_lang)\n",
        "print(f\"Input: {example_sentence}\")\n",
        "print(f\"Translation: {translated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCXBjYBgxajF",
        "outputId": "ef95e4a3-2309-47af-aae5-8d09b62cad12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: مرحبا بالعالم\n",
            "Translation: MR: Hello world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cP1c95nxxaZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}